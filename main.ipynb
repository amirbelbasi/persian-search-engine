{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139f86f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hazm in d:\\applications\\anaconda\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: nltk==3.3 in d:\\applications\\anaconda\\lib\\site-packages (from hazm) (3.3)\n",
      "Requirement already satisfied: six in d:\\applications\\anaconda\\lib\\site-packages (from nltk==3.3->hazm) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "#part 1\n",
      "#part 2\n"
     ]
    }
   ],
   "source": [
    "%pip install hazm\n",
    "\n",
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import codecs\n",
    "import json\n",
    "import collections\n",
    "\n",
    "print('#part 1')\n",
    "#f = open('foo.json')\n",
    "f = open('IR_data_news_12k.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "contents = []\n",
    "for i in data:\n",
    "    tmp = data[i]['content']\n",
    "    contents.append(tmp[0:len(tmp)-16])\n",
    "#print('#1')\n",
    "#for i in contents:\n",
    "#    print(i)\n",
    "#print()\n",
    "\n",
    "normalizer = Normalizer()\n",
    "for i in range(len(contents)):\n",
    "    contents[i] = normalizer.normalize(contents[i])\n",
    "#print('#2')\n",
    "#for i in contents:\n",
    "#    print(i)\n",
    "\n",
    "tokens = []\n",
    "for i in range(len(contents)):\n",
    "    tokens.append(word_tokenize(contents[i]))\n",
    "#print('#3')\n",
    "#for i in tokens:\n",
    "#    print(i)\n",
    "#    print()\n",
    "\n",
    "lemmatizer = Lemmatizer()\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens[i])):\n",
    "        tokens[i][j] = lemmatizer.lemmatize(tokens[i][j])\n",
    "#print('#4')\n",
    "#for i in tokens:\n",
    "#    print(i)\n",
    "#    print()\n",
    "\n",
    "\"\"\"\n",
    "stemmer = Stemmer()\n",
    "for i in range(len(tokens)):\n",
    "    for j in range(len(tokens[i])):\n",
    "        tokens[i][j] = stemmer.stem(tokens[i][j])\n",
    "print('#5')\n",
    "for i in tokens:\n",
    "    print(i)\n",
    "    print()\n",
    "\"\"\"\n",
    "\n",
    "f = codecs.open('stopwords.dat', encoding='utf-8')\n",
    "stopWords = []\n",
    "for l in f.readlines():\n",
    "    stopWords.append(l.strip('\\n'))\n",
    "f.close()\n",
    "#print('A#')\n",
    "#print(stopWords)\n",
    "#print()\n",
    "\n",
    "for i in range(len(stopWords)):\n",
    "    stopWords[i] = normalizer.normalize(stopWords[i])\n",
    "#print('B#')\n",
    "#print(stopWords)\n",
    "#print()\n",
    "\n",
    "for i in range(len(stopWords)):\n",
    "    stopWords[i] = lemmatizer.lemmatize(stopWords[i])\n",
    "#print('C#')\n",
    "#print(stopWords)\n",
    "#print()\n",
    "\n",
    "tmpTokens = []\n",
    "for i in range(len(tokens)):\n",
    "    tmpTokensDoc = []\n",
    "    for j in range(len(tokens[i])):\n",
    "        if(tokens[i][j] not in stopWords): tmpTokensDoc.append(tokens[i][j])\n",
    "    tmpTokens.append(tmpTokensDoc)\n",
    "tokens = tmpTokens.copy()\n",
    "#print('#5')\n",
    "#for i in tokens:\n",
    "#    for j in i:\n",
    "#        print(j)\n",
    "#    print('\\n+++++\\n')\n",
    "#print()\n",
    "\n",
    "print('#part 2')\n",
    "#uniqueTokens = set()\n",
    "#for i in tokens:\n",
    "#    for j in i:\n",
    "#        uniqueTokens.add(j)\n",
    "#sortedTokens = []\n",
    "#sortedTokens = sorted(list(uniqueTokens))\n",
    "#print('#6')\n",
    "#for i in sortedTokens:\n",
    "#    print(i)\n",
    "#print()\n",
    "\n",
    "positionalIndex = {}\n",
    "for j in range(len(tokens)):\n",
    "    for k in range(len(tokens[j])):\n",
    "        token = tokens[j][k]\n",
    "        if(token in positionalIndex):\n",
    "            oldDocFrequency = positionalIndex[token][0]\n",
    "            oldFrequencyInDoc = positionalIndex[token][1].copy()\n",
    "            oldPositionsInDoc = positionalIndex[token][2].copy()\n",
    "            if(j in oldFrequencyInDoc):\n",
    "                oldFrequencyInDoc[j] += 1\n",
    "                oldPositionsInDoc[j].append(k)\n",
    "                positionalIndex[token] = (oldDocFrequency, oldFrequencyInDoc.copy(), oldPositionsInDoc.copy())\n",
    "            else:\n",
    "                a = oldFrequencyInDoc.copy()\n",
    "                b = oldPositionsInDoc.copy()\n",
    "                a[j] = 1\n",
    "                b[j] = [k]\n",
    "                positionalIndex[token] = (oldDocFrequency+1, a.copy(), b.copy())\n",
    "        else:\n",
    "                a = dict()\n",
    "                b = dict()\n",
    "                a[j] = 1\n",
    "                b[j] = [k]\n",
    "                positionalIndex[token] = (1, a.copy(), b.copy())\n",
    "positionalIndex = collections.OrderedDict(sorted(positionalIndex.items())).copy()\n",
    "#print('#7')\n",
    "#print(positionalIndex['گیتی'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e85ef64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: طلای \"لیگ برتر\" ! والیبال\n",
      "\n",
      "Retrieved Doc #1 with DocID #974:\n",
      "Title: روایتی جالب از تیم قهرمان لیگ برتر واترپلو/ به نام شهیدی که تیم ملی و مدال را رها کرد و رفت!\n",
      "URL: https://www.farsnews.ir/news/14001208000567/روایتی-جالب-از-تیم-قهرمان-لیگ-برتر-واترپلو-به-نام-شهیدی-که-تیم-ملی-و\n",
      "\n",
      "Retrieved Doc #2 with DocID #6181:\n",
      "Title: بهترین بازیکنان نیم فصل لیگ برتر انگلیس +عکس\n",
      "URL: https://www.farsnews.ir/news/14001004000265/بهترین-بازیکنان-نیم-فصل-لیگ-برتر-انگلیس-عکس\n",
      "\n",
      "Retrieved Doc #3 with DocID #1388:\n",
      "Title: تجلیل از خانواده شهید حسن نوفلاح با تقدیم مدال قهرمانی\n",
      "URL: https://www.farsnews.ir/news/14001205000938/تجلیل-از-خانواده-شهید-حسن-نوفلاح-با-تقدیم-مدال-قهرمانی\n",
      "\n",
      "Retrieved Doc #4 with DocID #1361:\n",
      "Title: رئیس فدراسیون ووشو: تلاش می کنیم مربی خوشنامی برای بخش بانوان بیاوریم/با علی نژاد نسبتی ندارم\n",
      "URL: https://www.farsnews.ir/news/14001206000290/رئیس-فدراسیون-ووشو-تلاش-می-کنیم-مربی-خوشنامی-برای-بخش-بانوان-بیاوریم\n",
      "\n",
      "Retrieved Doc #5 with DocID #3103:\n",
      "Title: اوبامیانگ رسما قراردادش با آرسنال را فسخ کرد/در انتظار حضور ستاره گابنی در بارسا\n",
      "URL: https://www.farsnews.ir/news/14001112001065/اوبامیانگ-رسما-قراردادش-با-آرسنال-را-فسخ-کرد-در-انتظار-حضور-ستاره\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print('#part 3')\n",
    "import sys\n",
    "print('Enter your query: ', end = '')\n",
    "query = input()\n",
    "print()\n",
    "\n",
    "termsInQuery = []\n",
    "query = normalizer.normalize(query)\n",
    "termsInQuery = word_tokenize(query).copy()\n",
    "\n",
    "for i in range(len(termsInQuery)):\n",
    "    termsInQuery[i] = lemmatizer.lemmatize(termsInQuery[i])\n",
    "\n",
    "tmp = []\n",
    "for i in termsInQuery:\n",
    "    if(i not in stopWords or i == '!' or i == '\"' or i == '»' or i == '«'): tmp.append(i)\n",
    "termsInQuery = tmp.copy()\n",
    "\n",
    "#print('#8')\n",
    "#for i in termsInQuery:\n",
    "#    print(i)\n",
    "#print()\n",
    "\n",
    "for i in termsInQuery:\n",
    "    if i not in positionalIndex and i != '!' and i != '\"' and i != '»' and i != '«':\n",
    "        print('No Match Found!')\n",
    "        sys.exit()\n",
    "\n",
    "mustInclude = []\n",
    "mustNotInclude = []\n",
    "phrases = []\n",
    "negative = 0\n",
    "phrase = 0\n",
    "tmpPhrase = []\n",
    "for i in termsInQuery:\n",
    "    if i == '!':\n",
    "        negative = 1\n",
    "        continue\n",
    "    elif i == '«':\n",
    "        phrase = 1\n",
    "        continue\n",
    "    elif i == '»':\n",
    "        phrases.append(tmpPhrase)\n",
    "        tmpPhrase = []\n",
    "        phrase = 0\n",
    "        continue\n",
    "    elif negative == 1:\n",
    "        mustNotInclude.append(i)\n",
    "        negative = 0\n",
    "        continue\n",
    "    elif phrase == 1:\n",
    "        tmpPhrase.append(i)\n",
    "        continue\n",
    "    mustInclude.append(i)\n",
    "\n",
    "#print('#9')\n",
    "#for i in mustInclude:\n",
    "#    print(i)\n",
    "#print('+++++')\n",
    "#for i in mustNotInclude:\n",
    "#    print(i)\n",
    "#print('+++++')\n",
    "#for i in phrases:\n",
    "#    print(i)\n",
    "#print()\n",
    "\n",
    "tmp = []\n",
    "phraseAppearances = []\n",
    "if len(phrases) > 0:\n",
    "    for i in phrases:\n",
    "        pSumAppearances = dict()\n",
    "        if len(i) > 0: pSumAppearances = positionalIndex[i[0]][1].copy()\n",
    "        for j in range(len(i)-1):\n",
    "            a = pSumAppearances.copy()\n",
    "            b = positionalIndex[i[j+1]][1].copy()\n",
    "            pSumAppearances = {z: a.get(z, 0) + b.get(z, 0) for z in set(a) & set(b)}\n",
    "        pSumAppearances = collections.OrderedDict(sorted(pSumAppearances.items())).copy()\n",
    "        tmp.append(pSumAppearances)\n",
    "        phraseAppearances.append(dict())\n",
    "    for i in range(len(tmp)):\n",
    "        for j in tmp[i]:\n",
    "            dummy = []\n",
    "            for k in phrases[i]:\n",
    "                dummy.append(positionalIndex[k][2][j])\n",
    "            for k in dummy[0]:\n",
    "                continueFlag = 0\n",
    "                for l in range(len(dummy)-1):\n",
    "                    if (k+l+1) not in dummy[l+1]:\n",
    "                        continueFlag = 1\n",
    "                        break\n",
    "                if continueFlag == 1: continue\n",
    "                if j in phraseAppearances[i]: phraseAppearances[i][j] += 1\n",
    "                else: phraseAppearances[i][j] = 1\n",
    "\n",
    "#print('#10')\n",
    "#for i in phraseAppearances:\n",
    "#    print(i)\n",
    "#    print(len(i))\n",
    "#    print()\n",
    "#print()\n",
    "\n",
    "sumAppearances = dict()\n",
    "if len(mustInclude) > 0: sumAppearances = positionalIndex[mustInclude[0]][1].copy()\n",
    "for i in range(len(mustInclude)-1):\n",
    "    a = sumAppearances.copy()\n",
    "    b = positionalIndex[mustInclude[i+1]][1].copy()\n",
    "    sumAppearances = {z: a.get(z, 0) + b.get(z, 0) for z in set(a) | set(b)}\n",
    "sumAppearances = collections.OrderedDict(sorted(sumAppearances.items())).copy()\n",
    "\n",
    "allAppearances = dict()\n",
    "if len(phrases) > 0: allAppearances = phraseAppearances[0].copy()\n",
    "for i in range(len(phrases)-1):\n",
    "    a = allAppearances.copy()\n",
    "    b = phraseAppearances[i+1].copy()\n",
    "    allAppearances = {z: a.get(z, 0) + b.get(z, 0) for z in set(a) | set(b)}\n",
    "allAppearances = {z: allAppearances.get(z, 0) + sumAppearances.get(z, 0) for z in set(allAppearances) | set(sumAppearances)}\n",
    "allAppearances = collections.OrderedDict(sorted(allAppearances.items())).copy()\n",
    "\n",
    "varietyAppearances = dict()\n",
    "for i in allAppearances:\n",
    "    nAppearances = 0\n",
    "    for j in mustInclude:\n",
    "        if i in positionalIndex[j][1]: nAppearances += 1\n",
    "    for j in range(len(phrases)):\n",
    "        if i in phraseAppearances[j]: nAppearances += 1\n",
    "    varietyAppearances[i] = nAppearances\n",
    "varietyAppearances = collections.OrderedDict(sorted(varietyAppearances.items())).copy()\n",
    "\n",
    "result = []\n",
    "tmp = []\n",
    "for i in range(len(mustInclude)+len(phrases)):\n",
    "    result.append(list())\n",
    "    tmp.append(list())\n",
    "for i in allAppearances:\n",
    "    result[len(mustInclude)+len(phrases)-varietyAppearances[i]].append(i)\n",
    "for i in range(len(result)):\n",
    "    dummy = dict()\n",
    "    for j in result[i]:\n",
    "        dummy[j] = allAppearances[j]\n",
    "    tmp[i] = list(dict(sorted(dummy.items(), key=lambda item: item[1])).keys()).copy()\n",
    "    tmp[i].reverse()\n",
    "result = tmp.copy()\n",
    "\n",
    "if len(mustNotInclude) != 0:\n",
    "    tmp = []\n",
    "    for i in range(len(mustInclude)+len(phrases)):\n",
    "        tmp.append(list())\n",
    "    for i in mustNotInclude:\n",
    "        for j in range(len(result)):\n",
    "            for k in result[j]:\n",
    "                if k not in positionalIndex[i][1]: tmp[j].append(k)\n",
    "    result = tmp.copy()\n",
    "\n",
    "#print('#11')\n",
    "#for i in result:\n",
    "#    print(i)\n",
    "#    print(len(i))\n",
    "#    print()\n",
    "#print()\n",
    "\n",
    "res = []\n",
    "for i in result:\n",
    "    for j in i:\n",
    "        res.append(j)\n",
    "if len(res) == 0: print('No Match Found!')\n",
    "else:\n",
    "    i = 0\n",
    "    while i < min(5, len(res)):\n",
    "        print(f'Retrieved Doc #{i+1} with DocID #{res[i]}:')\n",
    "        a = data[str(res[i])]['title']\n",
    "        b = data[str(res[i])]['url']\n",
    "        print(f'Title: {a}')\n",
    "        print(f'URL: {b}')\n",
    "        print()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8aec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#phase 2\n"
     ]
    }
   ],
   "source": [
    "print('#phase 2')\n",
    "import math\n",
    "\n",
    "allTerms = list(positionalIndex.keys())\n",
    "\n",
    "docVectors = []\n",
    "for i in range(12202):\n",
    "    tmp = []\n",
    "    for t in allTerms:\n",
    "        if(i not in positionalIndex[t][1]):\n",
    "            tmp.append(0)\n",
    "            continue\n",
    "        f = positionalIndex[t][1][i]\n",
    "        tf = 1 + math.log(f, 10)\n",
    "        idf = math.log(12202/positionalIndex[t][0], 10)\n",
    "        tfidf = tf * idf\n",
    "        tmp.append(tfidf)\n",
    "    docVectors.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b2e639",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query: بالشت برزیل مالزی\n",
      "\n",
      "Method1:\n",
      "--------\n",
      "Retrieved Doc #1 with DocID #430:\n",
      "Title: دختر تاریخ ساز تنیس به مدال مسابقات تایلند نزدیک شد\n",
      "URL: https://www.farsnews.ir/news/14001218000206/دختر-تاریخ-ساز-تنیس-به-مدال-مسابقات-تایلند-نزدیک-شد\n",
      "\n",
      "Retrieved Doc #2 with DocID #9978:\n",
      "Title: اعطای مدال قدس به نماینده دولت مالزی توسط دانشجویان بسیجی\n",
      "URL: https://www.farsnews.ir/news/14000920000114/اعطای-مدال-قدس-به-نماینده-دولت-مالزی-توسط-دانشجویان-بسیجی\n",
      "\n",
      "Retrieved Doc #3 with DocID #5682:\n",
      "Title: لابی صهیونیست‌ها در ورزش/هشدار جدی به مالزی به خاطر حمایت از فلسطین+عکس\n",
      "URL: https://www.farsnews.ir/news/14001011000158/لابی-صهیونیست‌ها-در-ورزش-هشدار-جدی-به-مالزی-به-خاطر-حمایت-از-فلسطین\n",
      "\n",
      "Retrieved Doc #4 with DocID #0:\n",
      "Title: اعلام زمان قرعه کشی جام باشگاه های فوتسال آسیا\n",
      "URL: https://www.farsnews.ir/news/14001224001005/اعلام-زمان-قرعه-کشی-جام-باشگاه-های-فوتسال-آسیا\n",
      "\n",
      "Retrieved Doc #5 with DocID #2003:\n",
      "Title: قهرمانی زیر 23 سال آسیا| ایران با ازبکستان، قطر و ترکمنستان رقیب شد\n",
      "URL: https://www.farsnews.ir/news/14001128000268/قهرمانی-زیر-23-سال-آسیا|-ایران-با-ازبکستان-قطر-و-ترکمنستان-رقیب-شد\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nprint('Method2:')\\nprint('--------')\\ni = 0\\nwhile i < K:\\n    print(f'Retrieved Doc #{i+1} with DocID #{res2[i]}:')\\n    a = data[str(res2[i])]['title']\\n    b = data[str(res2[i])]['url']\\n    print(f'Title: {a}')\\n    print(f'URL: {b}')\\n    print()\\n    i += 1\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Enter your query: ', end = '')\n",
    "query = input()\n",
    "print()\n",
    "termsInQuery = []\n",
    "query = normalizer.normalize(query)\n",
    "termsInQuery = word_tokenize(query).copy()\n",
    "for i in range(len(termsInQuery)):\n",
    "    termsInQuery[i] = lemmatizer.lemmatize(termsInQuery[i])\n",
    "tmp = []\n",
    "for i in termsInQuery:\n",
    "    if(i not in stopWords or i == '!' or i == '\"' or i == '»' or i == '«'): tmp.append(i)\n",
    "termsInQuery = tmp.copy()\n",
    "for i in termsInQuery:\n",
    "    if i not in positionalIndex and i != '!' and i != '\"' and i != '»' and i != '«':\n",
    "        print('No Match Found!')\n",
    "        sys.exit()\n",
    "\n",
    "#print('#1')\n",
    "#for i in termsInQuery:\n",
    "#    print(i)\n",
    "#print()\n",
    "\n",
    "queryVector = []\n",
    "for t in allTerms:\n",
    "    if(t not in termsInQuery):\n",
    "        queryVector.append(0)\n",
    "        continue\n",
    "    cnt = 0\n",
    "    for j in termsInQuery:\n",
    "        if(t == j): cnt += 1\n",
    "    f = cnt\n",
    "    tf = 1 + math.log(f, 10)\n",
    "    idf = math.log(12202/positionalIndex[t][0], 10)\n",
    "    tfidf = tf * idf\n",
    "    queryVector.append(tfidf)\n",
    "\n",
    "#print('#2')\n",
    "#for i in range(len(allTerms)):\n",
    "#    print(f'{allTerms[i]} ==> {queryVector[i]}')\n",
    "\n",
    "uniqueTermsInQuery = set()\n",
    "for i in termsInQuery:\n",
    "    uniqueTermsInQuery.add(i)\n",
    "uniqueTermsInQuery = list(uniqueTermsInQuery)\n",
    "\n",
    "def method1(a, b):\n",
    "    dotProduct = 0\n",
    "    for i in range(len(a)):\n",
    "        dotProduct += a[i] * b[i]\n",
    "    absA = 0\n",
    "    absB = 0\n",
    "    for i in range(len(a)):\n",
    "        absA += a[i] * a[i]\n",
    "        absB += b[i] * b[i]\n",
    "    absA = math.sqrt(absA)\n",
    "    absB = math.sqrt(absB)\n",
    "    return dotProduct/(absA*absB)\n",
    "\n",
    "def method2(a, b):\n",
    "    c = 0.0\n",
    "    for i in range(len(a)):\n",
    "        if(a[i]!=0 and b[i]!=0): c+=1\n",
    "    d = 0.0\n",
    "    for i in range(len(a)):\n",
    "        if(a[i]!=0 or b[i]!=0): d+=1\n",
    "    if(d == 0): return 0\n",
    "    return c/d\n",
    "\n",
    "similarities1 = []\n",
    "similarities2 = []\n",
    "for i in range(len(docVectors)):\n",
    "    similarities1.append(method1(queryVector, docVectors[i]))\n",
    "    #similarities2.append(method2(queryVector, docVectors[i]))\n",
    "\n",
    "K = 5\n",
    "import heapq\n",
    "res1 = heapq.nlargest(K, range(len(similarities1)), key=similarities1.__getitem__).copy()\n",
    "#res2 = heapq.nlargest(K, range(len(similarities2)), key=similarities2.__getitem__).copy()\n",
    "\n",
    "print('Method1:')\n",
    "print('--------')\n",
    "i = 0\n",
    "while i < K:\n",
    "    print(f'Retrieved Doc #{i+1} with DocID #{res1[i]}:')\n",
    "    a = data[str(res1[i])]['title']\n",
    "    b = data[str(res1[i])]['url']\n",
    "    print(f'Title: {a}')\n",
    "    print(f'URL: {b}')\n",
    "    print()\n",
    "    i += 1\n",
    "\"\"\"\n",
    "print('Method2:')\n",
    "print('--------')\n",
    "i = 0\n",
    "while i < K:\n",
    "    print(f'Retrieved Doc #{i+1} with DocID #{res2[i]}:')\n",
    "    a = data[str(res2[i])]['title']\n",
    "    b = data[str(res2[i])]['url']\n",
    "    print(f'Title: {a}')\n",
    "    print(f'URL: {b}')\n",
    "    print()\n",
    "    i += 1\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
